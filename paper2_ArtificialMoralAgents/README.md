### Can (ro)bots really be moral? Marta Pienkosz

The mind, long a mystery, remains a puzzle scientists today are still trying to solve. Recent scholarly efforts have taken a new path, shifting from its biological origins to thinking of the mind as an information processing network. Ray Kurzweil, a strong believer in the idea that a properly programmed computer can replicate the mind, presents an intriguing narrative. According to Kurzweil, by around 2029, machine intelligence is predicted to surpass the collective human intelligence, marking the beginning of the Singularity (Wallach and Allen 57). This pivotal moment promises a future filled with unpredictability, driven by rapid machine intelligence growth, raising vital questions about its societal impact. Although significant progress has been made, the human brain still surpasses computational power in two critical dimensions: conscious understanding and free will.

However, the intriguing prospect of engineering a moral dimension into artificial moral agents (AMAs) prompts us to consider how these two aspects, if ever computationally developed, could be integrated— how can we safeguard genuine moral agency in an era increasingly dominated by artificial intelligence?

In relation to free will, the dynamic between humans and robots in this context takes on a common nature, giving rise to the inevitable conflict between an agent's internal goals and that of others. The very essence of moral agency lies in the maximization of choices, which in turn leads to the increased potential for ethical dilemmas. Even within a deterministic framework for action, it can be argued that ethics inherently demands open-ended decision-making (Wallach and Allen 62).

To illustrate the contemporary relevance of these deliberations, it’s worth considering the widely known 'trolley problem' in the 21st century. In this scenario, two children abruptly appear before a self-driving car. The algorithm behind the car must make a rapid judgment call: should it swerve into oncoming traffic, risking a collision with a truck, and accept a 70% chance of killing the car owner? In a similar real-life scenario, many human drivers, unaware of the impending collision, might instinctively opt to save the children.

In contrast, a well-designed AMA could possess a unique ability to assess multiple options and consider a variety of evaluative perspectives, all informed by its deterministic system and its capacity to adapt to the 'transition rules'. As AMAs make choices and navigate ethical challenges, they wield a unique influence on the prevailing moral norms dictated by their training data, thus shaping the existing moral ecosystem within their operational context (Wallach and Allen 62). In essence, AMAs could become dynamic participants in the ongoing evolution of the framework in which they function. However, a pressing question remains: when programming ethics into a deterministic system, whose morality or what moral principles should it adhere to?

The question of conscious understanding adds yet another layer of complexity: is it truly essential to possess it in order to be recognized as a moral agent? In the context of evaluating a program's ‘genuine’ understanding, robots, with their growing sophistication, have exhibited the capacity to adapt effectively to both social and physical environments (Wallach and Allen 66).

Consciousness, on the other hand, presents a far more complex challenge. The current consensus suggests that consciousness is closely intertwined with neural processes, rendering the pursuit of artificial consciousness through computational means somewhat redundant (Wallach and Allen 67). Nevertheless, comprehending consciousness in terms of its functionality offers a more promising approach. The emerging subfield of machine consciousness within AI aims to construct internal representations of a robot's world and behavior, encompassing aspects such as self-awareness, imagination, and even emotions, potentially giving rise to consciousness-like phenomena. Nevertheless, some philosophers remain skeptical and argue that the achievement of functional equivalence in computers will never fully meet the criteria for the profound nature of consciousness (Wallach and Allen 68). In the pragmatic realm of AMAs, the central focus is on functionality, and this consideration takes precedence in the development of ethical artificial agents.

As robots increasingly approach human capabilities, they still exhibit significant cognitive limitations. Unlike humans, computers lack the depth of understanding and consciousness, impacting their ability to grasp nuances and make sensitive judgments. It's vital to recognize the fundamental distinctions between humans, evolved from a biochemical platform rooted in emotional intelligence, and AI, developed on a logical basis. Human decisions, in contrast, are not purely rational. When it comes to computers meeting higher standards, what those standards should be remains a question, and translating them into algorithms presents an overwhelming challenge. Nevertheless, the effort to bridge theory and practical implementation promises to be instructive for both ethicists and roboticists.


### Works Cited

* Wallach, Wendell, and Colin Allen. Moral Machines: Teaching Robots Right from Wrong. Oxford University Press, 2010.
